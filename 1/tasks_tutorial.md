### **Общее замечание**
Во всех заданиях используется **логарифм по основанию 2** (`log`), как указано в пособии (Определение 1.2). Энтропия вычисляется по формуле Шеннона (Определение 1.3):
$$H(X) = -\sum_{i=1}^{n} p_i \cdot \log p_i$$
Помните, что по определению `0 · log 0 = 0`.

---

### **Задание 1: Биномиальное распределение (Энтропия числа успехов в n испытаниях)**

**Цель:** Исследовать, как меняется энтропия числа появлений события A в серии из `n` независимых испытаний в зависимости от вероятности `p` этого события.

**Шаги выполнения:**

1.  **Зафиксируйте параметр `n`.** Пусть, например, `n = 2` (значение должно быть задано преподавателем, для примера возьмем небольшое).

2.  **Составьте общую вероятностную схему.**
    Случайная величина `X` — число появлений события A (число "успехов") в `n` испытаниях. Она может принимать значения от `0` до `n`.
    *   **Формула для расчета вероятностей:** Вероятность получить ровно `k` успехов вычисляется по формуле Бернулли:
        $$P(X=k) = C_n^k \cdot p^k \cdot (1-p)^{n-k}$$
        где `C_n^k` — биномиальный коэффициент (число сочетаний из `n` по `k`).

3.  **Для каждого значения `p` из диапазона [0, 1] (например, с шагом 0.1):**
    *   а) **Рассчитайте вероятности** для каждого возможного значения `k` (0, 1, ..., n).
    *   б) **Рассчитайте энтропию `H(p)`** для полученного набора вероятностей по формуле Шеннона.
    *   в) **Занесите результаты в таблицу.**

    **Пример таблицы для n=2:**
    | `p` | `P(X=0) = (1-p)^2` | `P(X=1) = 2p(1-p)` | `P(X=2) = p^2` | `H(p) = -[P(0)logP(0) + P(1)logP(1) + P(2)logP(2)]` |
    | :-- | :--- | :--- | :--- | :--- |
    | 0.0 | 1.0 | 0.0 | 0.0 | `-(1*log1 + 0 + 0) = 0` |
    | 0.1 | 0.81 | 0.18 | 0.01 | `-(0.81*log0.81 + 0.18*log0.18 + 0.01*log0.01) ≈ 0.94` |
    | 0.2 | 0.64 | 0.32 | 0.04 | ... *(рассчитать)* |
    | ... | ... | ... | ... | ... |
    | 0.5 | 0.25 | 0.5 | 0.25 | `-(0.25*log0.25 + 0.5*log0.5 + 0.25*log0.25) = 1.5` |
    | ... | ... | ... | ... | ... |
    | 1.0 | 0.0 | 0.0 | 1.0 | 0 |

4.  **Постройте график функции `H(p)`.**
    *   На оси **X** отложите значения вероятности `p`.
    *   На оси **Y** отложите рассчитанные значения энтропии `H(p)`.
    *   Аккуратно соедините точки плавной кривой.
    *   **Подпишите оси** и дайте графику название, например, "Зависимость энтропии H(p) для биномиального распределения (n=2)".

5.  **Проанализируйте график и определите экстремумы.**
    *   **Максимум:** Энтропия достигает максимального значения при `p = 0.5`. Это точка наибольшей неопределенности, когда исход эксперимента предсказать сложнее всего. Для `n=2` `H(0.5) = 1.5`.
    *   **Минимум:** Энтропия равна `0` на концах отрезка, при `p=0` и `p=1`. Это точки полной определенности: мы абсолютно уверены, что событие A никогда не произойдет (`p=0`) или произойдет всегда (`p=1`).

---

### **Задание 2: Геометрическое распределение (Энтропия номера первого успеха)**

**Цель:** Исследовать, как меняется энтропия номера испытания, в котором событие A произойдет в первый раз.

**Шаги выполнения:**

1.  **Составьте вероятностную схему.**
    Случайная величина `Y` — номер первого успешного испытания. Она может принимать значения `1, 2, 3, ...` до бесконечности.
    *   **Формула для расчета вероятностей:** Вероятность того, что первый успех произойдет на `k`-м испытании:
        $$P(Y=k) = (1-p)^{k-1} \cdot p$$

2.  **Для каждого значения `p` из диапазона (0, 1] (например, с шагом 0.1):**
    *   а) **Сформируйте усеченное распределение.** Так как ряд бесконечный, нужно ограничиться конечным числом слагаемых. суммируйте вероятности, пока они не станут очень маленькими (например, меньше `0.001`). Для `p=0.5` можно взять первые 10-15 испытаний. Для `p=0.1` понадобится больше испытаний (около 50).
    *   б) **Нормализуйте вероятности.** Убедитесь, что сумма вероятностей в вашем усеченном распределении равна (или очень близка к) 1. Если сумма `S < 1`, можно проигнорировать это для грубой оценки или мысленно добавить оставшуюся вероятность к последнему члену, но для аккуратности лучше взять больше слагаемых.
    *   в) **Рассчитайте энтропию `H(p)`** для полученного набора вероятностей.
    *   г) **Занесите результаты в таблицу.**

    **Пример таблицы для p=0.5 (первые 5 испытаний):**
    | `k` | `P(Y=k) = 0.5^(k-1)*0.5` |
    | :-- | :--- |
    | 1 | 0.5 |
    | 2 | 0.25 |
    | 3 | 0.125 |
    | 4 | 0.0625 |
    | 5 | 0.03125 |
    | ... | ... |
    | **Сумма (S)** | ~0.96875 (для 5 trials) |
    | **H(0.5)** | `-(0.5*log0.5 + 0.25*log0.25 + ... ) ≈ 1.78` |

3.  **Постройте график функции `H(p)`.**
    *   На оси **X** отложите значения вероятности `p`.
    *   На оси **Y** отложите рассчитанные значения энтропии `H(p)`.
    *   Аккуратно соедините точки плавной кривой.
    *   **Подпишите оси** и дайте графику название.

4.  **Проанализируйте график и определите экстремумы.**
    *   **Максимум:** Энтропия достигает максимума при некотором значении `p` между 0.2 и 0.4 (точное значение зависит от степени усечения ряда). Это точка наибольшей неопределенности момента первого успеха.
    *   **Минимум:** Энтропия **стремится к 0** при `p -> 1` (успех почти наверняка случится в первом же испытании) и при `p -> 0` (успех придется ждать бесконечно долго, но вероятность каждого конкретного `k` очень мала, и неопределенность также мала).

---

### **Задание 3: Гипергеометрическое распределение (Энтропия числа стандартных деталей в выборке)**

**Цель:** Исследовать, как меняется энтропия числа стандартных изделий в выборке объема `m` из партии объема `n` в зависимости от общего числа стандартных изделий `k` в партии.

**Шаги выполнения:**

1.  **Зафиксируйте параметры `n` и `m`.** Пусть, например, `n = 5`, `m = 2`.

2.  **Составьте общую вероятностную схему.**
    Случайная величина `Z` — число стандартных изделий в выборке. Она может принимать значения от `max(0, m - (n - k))` до `min(m, k)`.
    *   **Формула для расчета вероятностей** (вероятность того, что ровно `i` изделий в выборке стандартные):
        $$P(Z=i) = \frac{C_k^i \cdot C_{n-k}^{m-i}}{C_n^m}$$
        где `C_a^b` — число сочетаний.

3.  **Для каждого значения `k` из диапазона [0, n] (k — число стандартных изделий в партии):**
    *   а) **Определите допустимые значения `i`** для данного `k`: от `i_min = max(0, m - (n - k))` до `i_max = min(m, k)`.
    *   б) **Рассчитайте вероятности** для каждого допустимого `i`.
    *   в) **Рассчитайте энтропию `H(k)`** для полученного набора вероятностей.
    *   г) **Занесите результаты в таблицу.**

    **Пример таблицы для n=5, m=2, k=3:**
    *   `i_min = max(0, 2 - (5-3)) = max(0,0)=0`
    *   `i_max = min(2,3)=2` -> `i` может быть 0, 1, 2.
    *   `P(Z=0) = C_3^0 * C_2^2 / C_5^2 = 1 * 1 / 10 = 0.1`
    *   `P(Z=1) = C_3^1 * C_2^1 / C_5^2 = 3 * 2 / 10 = 0.6`
    *   `P(Z=2) = C_3^2 * C_2^0 / C_5^2 = 3 * 1 / 10 = 0.3`
    *   `H(3) = -(0.1*log0.1 + 0.6*log0.6 + 0.3*log0.3) ≈ 1.30`

4.  **Постройте график функции `H(k)`.**
    *   На оси **X** отложите значения `k` (число стандартных изделий в партии).
    *   На оси **Y** отложите рассчитанные значения энтропии `H(k)`.
    *   Соедините точки.
    *   **Подпишите оси** и дайте графику название.

5.  **Проанализируйте график и определите экстремумы.**
    *   **Максимум:** Энтропия максимальна, когда количество стандартных изделий `k` в партии примерно равно `n/2`. В этом случае неопределенность результата выбора наибольшая. Для `n=5, m=2` максимум будет near `k=2` или `k=3`.
    *   **Минимум:** Энтропия равна `0` на концах, при `k=0` (в партии нет стандартных деталей, в выборке их всегда 0) и при `k=n` (все детали стандартные, в выборке их всегда `m`). В этих случаях исход предопределен.