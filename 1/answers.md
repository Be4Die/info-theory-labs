Конечно, вот подробные, но сжатые ответы на контрольные вопросы к лабораторной работе №1, составленные на основе предоставленного учебного пособия.

### **Ответы на контрольные вопросы (ЛР №1)**

**1. Количество информации в сообщении; основные свойства.**

*   **Определение:** Количество информации, содержащееся в сообщении \( x \), которое происходит с вероятностью \( p(x) \), определяется по формуле:
    \[
    h(x) = -\log p(x)
    \]
    (Основание логарифма обычно равно 2, и информация измеряется в битах).

*   **Основные свойства:**
    *   **Неотрицательность:** \( h(x) \geq 0 \). Чем менее вероятно событие, тем больше количество информации в сообщении о его появлении.
    *   **Убывающая функция вероятности:** Чем больше \( p(x) \), тем меньше \( h(x) \). Достоверное событие (\( p(x) = 1 \)) не несет информации (\( h(x) = 0 \)).
    *   **Аддитивность для независимых событий:** Количество информации в сообщении о двух независимых событиях равно сумме количеств информации о каждом из них: \( h(xy) = h(x) + h(y) \).

**2. Количество информации в сообщении относительно другого сообщения; основные свойства.**

*   **Определение (взаимная информация):** Это мера того, сколько информации одно сообщение (или случайная величина) содержит о другом. Формально, количество информации, которое несет схема \( Y \) относительно схемы \( X \):
    \[
    I(Y, X) = H(Y) - H(Y | X)
    \]
    где \( H(Y) \) — энтропия \( Y \), а \( H(Y|X) \) — условная энтропия \( Y \) при известном \( X \). Это показывает, насколько знание \( X \) уменьшает неопределенность \( Y \).

*   **Основные свойства:**
    *   **Симметричность:** \( I(Y, X) = I(X, Y) \). Информация взаимна.
    *   **Неотрицательность:** \( I(Y, X) \geq 0 \). Знание одной величины не может увеличить неопределенность другой.
    *   **Связь с энтропией:** \( I(Y, X) = H(X) + H(Y) - H(XY) \).

**3. Энтропия, условная энтропия; основные свойства.**

*   **Энтропия (H(X)):** Это среднее количество информации, приходящееся на одно сообщение от источника, или мера неопределенности вероятностной схемы *до* проведения испытания.
    \[
    H(X) = -\sum_{i=1}^{n} p_i \cdot \log p_i
    \]
    *   **Свойства энтропии:**
        1.  **Неотрицательность:** \( H(X) \geq 0 \).
        2.  **Максимальное значение:** Для схемы с \( n \) исходами энтропия максимальна и равна \( \log n \), когда все события равновероятны (\( p_i = 1/n \)). Это соответствует максимальной неопределенности.
        3.  **Минимальное значение:** \( H(X) = 0 \), когда вероятность одного из событий равна 1, а остальных — 0. Это соответствует полной определенности исхода.
        4.  **Пример из пособия:** Для схемы с двумя исходами (0 и 1) энтропия \( H(p) = -p \log p - (1-p)\log(1-p) \) максимальна (1 бит) при \( p=0.5 \) и равна 0 при \( p=0 \) или \( p=1 \).

*   **Условная энтропия (H(Y|X)):** Это мера средней неопределенности случайной величины \( Y \) *после* того, как значение случайной величины \( X \) стало известно.
    \[
    H(Y | X ) = -\sum_{i=1}^{n} \sum_{j=1}^{m} p(x_i) p(y_j | x_i) \log p(y_j | x_i)
    \]
    *   **Свойство:** Условная энтропия всегда меньше или равна безусловной: \( H(Y|X) \leq H(Y) \). Знание \( X \) в среднем не увеличивает неопределенность \( Y \).

**4. Взаимная информация вероятностных схем; основные свойства.**

*   **Определение:** Взаимная информация \( I(X,Y) \) для двух вероятностных схем — это величина, показывающая, насколько знания о одной схеме уменьшают неопределенность другой. Как было указано в п.2:
    \[
    I(X, Y) = H(X) - H(X | Y) = H(Y) - H(Y | X)
    \]

*   **Основные свойства (дополнение к п.2):**
    *   **Связь с совместной энтропией:** \( I(X, Y) = H(X) + H(Y) - H(X, Y) \).
    *   **Если схемы независимы:** \( I(X, Y) = 0 \). Знание одной величины не дает информации о другой.
    *   **Пределы:** \( 0 \leq I(X, Y) \leq \min(H(X), H(Y)) \). Взаимная информация не может превысить энтропию ни одной из величин.